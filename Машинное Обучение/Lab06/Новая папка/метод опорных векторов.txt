Задание:

Решите задачу классификации по вашему варианту из предыдущей работы следующими методами:

метод опорных векторов

При решении задачи подберите с использованием кросс-проверки следующие параметры алгоритмов:

параметр C для SVM

Как отработали методы для вашей задачи? Почему получились такие результаты?

мой датасет под который нужно будет сделать задания - exam_регр.csv, 
с полями - Id, age, years_of_experience, lesson_price, qualification, physics, chemistry, biology, english, geography, history, mean_exam_points.
в датасете делимитер это ; а не ,

пример:

метод опорных векторов:
```
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
 # Воспользуемся настройками по умолчанию библиотеки Seaborn
import seaborn as sns
sns.set()

# Загрузить библиотеки
from sklearn import datasets
from sklearn.preprocessing import StandardScaler 
from sklearn.svm import LinearSVC

# Загрузить данные всего с двумя классами и двумя признаками 
iris = datasets.load_iris()
features = iris.data[:300,:2] 
target = iris.target[:300]
# Стнадартизировать признаки 
scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)
# Создать опорно-векторный классификатор 
svc = LinearSVC(C=1.0, dual=True)
# Натренировать модель
model = svc.fit(features_standardized, target)

# Загрузить библиотеку
from matplotlib import pyplot as pit


# Вывести точки данных на график и расцветить, используя их класс 
color = ["red" if C == 0 else "blue" for C in target] 
pit.scatter(features_standardized[:,0], features_standardized[:,1], c=color)

# Создать гиперплоскость 
w = svc.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-2.5, 2.5)
yy = a * xx-(svc.intercept_[0]) / w[1]

# Начертить гиперплоскость 
pit.plot(xx, yy)
pit. show ()
```

SVM-ядра:
```
# Загрузить библиотеки 
from sklearn.svm import SVC 
from sklearn import datasets
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler 
import numpy as np
# Задать начальное значение рандомизации 
np.random.seed(0)
# Сгенерировать два признака 
features = np.random.randn(200, 2)
# Cгенерируем линейно разделимые классы
target_xor = np.logical_xor(features[:, 0] > 0 , features[:, 1] > 0 ) 
target = np.where(target_xor, 0, 1)

# Создать опорно-векторную машину  с радиально-базисным функциональным ядром (RBF-ядром) 
svc = SVC(kernel="rbf", random_state=0, gamma=1, C=1)
# Натренировать классификатор 
model = svc.fit(features, target)

# Выведем на график наблюдения и гиперплоскость границы решения 
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as pit

def plot_decision_regions(X, y, classifier): 
    cmap = ListedColormap(("red", "blue"))
    xx1, xx2 = np.meshgrid(np.arange(-3, 3, 0.02), np.arange(-3, 3, 0.02))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.1, cmap=cmap)
    for idx, cl in enumerate(np.unique(y)): 
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, color=cmap(idx), label=cl)

# Создать опорно-векторный классификатор с линейным ядром 
svc_linear = SVC(kernel="linear", random_state=0, C=1)
# Натренируем модель 
svc_linear.fit(features, target)
SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=0, shrinking=True, 
tol=0.001, verbose=False)

plot_decision_regions(features, target, classifier=svc_linear) 
pit.show()

# Создать опорно-векторную машину с радиально-базисным функциональным ядром (RBF-ядром) 
svc = SVC(kernel="rbf", random_state=0, gamma=1, C=1)
# Натренировать классификатор 
model = svc.fit(features, target)

# Вывести на график наблюдения и гиперплоскость 
plot_decision_regions(features, target, classifier=svc) 
pit. show ()
```